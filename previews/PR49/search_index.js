var documenterSearchIndex = {"docs":
[{"location":"latent_slice/#latent","page":"Latent Slice Sampling","title":"Latent Slice Sampling","text":"","category":"section"},{"location":"latent_slice/#Introduction","page":"Latent Slice Sampling","title":"Introduction","text":"Latent slice sampling is a recent vector-valued slice sampling algorithm proposed by Li and Walker[LW2023]. Unlike other slice sampling algorithms, it treats the \"search intervals\" as auxiliary variables and adapts them along the samples from the log-target in a Gibbs-type scheme.","category":"section"},{"location":"latent_slice/#Description","page":"Latent Slice Sampling","title":"Description","text":"Specifically, the extended joint density of the latent slice sampler is as follows:\n\n    p(x t s l) = pi(x)  p(s)  operatornameUniformleft(t 0 pileft(xright)right)  operatornameUniformleft(l  x - s2 x + s2right)\n\nwhere y is the parameters of the log-target pi, s is the width of the search interval and l is the centering of the search interval relative to y. Naturally, the sampler operates as a blocked-Gibbs sampler \n\nbeginaligned\nl_n sim operatornameUniformleft(l  x_n-1 - s_n-12 x_n-1 + s_n-12right) \ns_n sim p(s mid x_n-1 l_n) \nt_n sim operatornameUniformleft(0 pileft(x_n-1right)right) \nx_n sim operatornameUniformleftx mid pileft(xright)  t_nright\nendaligned\n\nWhen x_n is updated using the usual shrinkage procedure of Neal[N2003], s_n and l_n are used to form the initial search window. (s_n is the width of the window and l_n is its center point.) Therefore, the latent slice sampler can be regarded as an automatic tuning mechanism of the \"initial interval\" of slice samplers.\n\nThe only tunable parameter of the algorithm is then the distribution of the width p(s). For this, Li and Walker recommend\n\n    p(s beta) = operatornameGamma(s 2 beta)\n\nwhere beta is a tunable parameter. The use of the gamma distribution is somewhat important since the complete conditional p(s mid y l) needs to be available in closed-form for efficiency.  (It is a truncated exponential distribution in case of the gamma.) Therefore, we only provide control over beta.\n\ninfo: Info\nThe kernel corresponding to this sampler is defined on an augmented state space and cannot directly perform a transition on x. This also means that the corresponding kernel is not reversible with respect to x.","category":"section"},{"location":"latent_slice/#Interface","page":"Latent Slice Sampling","title":"Interface","text":"[LW2023]: Li, Y., & Walker, S. G. (2023). A latent slice sampling algorithm. Computational Statistics & Data Analysis, 179, 107652.\n\n[N2003]: Neal, R. M. (2003). Slice sampling. The annals of statistics, 31(3), 705-767.","category":"section"},{"location":"latent_slice/#SliceSampling.LatentSlice","page":"Latent Slice Sampling","title":"SliceSampling.LatentSlice","text":"LatentSlice(beta)\n\nLatent slice sampling algorithm by Li and Walker[LW2023].\n\nArguments\n\nbeta::Real: Beta parameter of the Gamma distribution of the auxiliary variables.\n\nKeyword Arguments\n\nmax_proposals::Int: Maximum number of proposals allowed until throwing an error (default: 10000).\n\n\n\n\n\n","category":"type"},{"location":"gibbs_polar/#polar","page":"Gibbsian Polar Slice Sampling","title":"Gibbsian Polar Slice Sampling","text":"","category":"section"},{"location":"gibbs_polar/#Introduction","page":"Gibbsian Polar Slice Sampling","title":"Introduction","text":"Gibbsian polar slice sampling (GPSS) is a multivariate slice sampling algorithm proposed by P. Schär, M. Habeck, and D. Rudolf[SHR2023]. It is an computationally efficient variant of polar slice sampler previously proposed by Roberts and Rosenthal[RR2002]. Unlike other slice sampling algorithms, it operates a Gibbs sampler over polar coordinates, reminiscent of the elliptical slice sampler (ESS). Due to the involvement of polar coordinates, GPSS only works reliably on more than one dimension. However, unlike ESS, GPSS is applicable to any target distribution.","category":"section"},{"location":"gibbs_polar/#Description","page":"Gibbsian Polar Slice Sampling","title":"Description","text":"For a d-dimensional target distribution pi, GPSS utilizes the following augmented target distribution:\n\nbeginaligned\n    p(x T)      = varrho_pi^(0)(x) varrho_pi^(1)(x)  operatornameUniformleft(T 0 varrho^1(x)right) \n    varrho_pi^(0)(x) = lVert x rVert^1 - d \n    varrho_pi^(1)(x) = lVert x rVert^d-1 pileft(xright)\nendaligned\n\nAs described in Appendix A of the GPSS paper, sampling from varrho^(1)(x) in polar coordinates magically targets the augmented target distribution.\n\nIn a high-level view, GPSS operates a Gibbs sampler in the following fashion:\n\nbeginaligned\nT_n      sim operatornameUniformleft(0 varrho^(1)left(x_n-1right)right) \ntheta_n sim operatornameUniformleft theta in mathbbS^d-1 mid varrho^(1)left(r_n-1 thetaright)  T_n right \nr_n      sim operatornameUniformleft r in mathbbR_geq 0 mid varrho^(1)left(r theta_nright)  T_n right \nx_n      = theta_n r_n\nendaligned\n\nwhere T_n is the usual acceptance threshold auxiliary variable, while theta and r are the sampler states in polar coordinates. The Gibbs steps on theta and r are implemented through specialized shrinkage procedures.\n\nThe only tunable parameter of the algorithm is the size of the search interval (window) of the shrinkage sampler for the radius variable r.\n\nwarning: Warning\nA limitation of the current implementation of GPSS is that the acceptance rate exhibits a heavy tail. That is, occasionally, a single transition might take an excessive amount of time.\n\ninfo: Info\nThe kernel corresponding to this sampler is defined on an augmented state space and cannot directly perform a transition on x. This also means that the corresponding kernel is not reversible with respect to x.","category":"section"},{"location":"gibbs_polar/#Interface","page":"Gibbsian Polar Slice Sampling","title":"Interface","text":"","category":"section"},{"location":"gibbs_polar/#Demonstration","page":"Gibbsian Polar Slice Sampling","title":"Demonstration","text":"As illustrated in the original paper, GPSS shows good performance on heavy-tailed targets despite being a multivariate slice sampler. Consider a 10-dimensional Student-t target with 1-degree of freedom (this corresponds to a multivariate Cauchy):\n\nusing Distributions\nusing Turing\nusing SliceSampling\nusing LinearAlgebra\nusing Plots\n\n@model function demo()\n    x ~ MvTDist(1, zeros(10), Matrix(I,10,10))\nend\nmodel = demo()\n\nn_samples = 1000\ninitial_params = InitFromParams((x = ones(10),))\nlatent_chain = sample(model, externalsampler(LatentSlice(10)), n_samples; initial_params=initial_params)\npolar_chain = sample(model, externalsampler(GibbsPolarSlice(10)), n_samples; initial_params=initial_params)\n\nl = @layout [a; b]\np1 = Plots.plot(1:n_samples, latent_chain[:,1,:], ylims=[-10,10], label=\"LSS\")\np2 = Plots.plot(1:n_samples, polar_chain[:,1,:],  ylims=[-10,10], label=\"GPSS\")\nplot(p1, p2, layout = l)\nsavefig(\"student_latent_gpss.svg\")\n\n(Image: )\n\nClearly, GPSS is better at exploring the deep tails compared to the latent slice sampler (LSS) despite having a similar per-iteration cost.\n\n[SHR2023]: Schär, P., Habeck, M., & Rudolf, D. (2023, July). Gibbsian polar slice sampling. In International Conference on Machine Learning.\n\n[RR2002]: Roberts, G. O., & Rosenthal, J. S. (2002). The polar slice sampler. Stochastic Models, 18(2), 257-280.","category":"section"},{"location":"gibbs_polar/#SliceSampling.GibbsPolarSlice","page":"Gibbsian Polar Slice Sampling","title":"SliceSampling.GibbsPolarSlice","text":"GibbsPolarSlice(w; max_proposals)\n\nGibbsian polar slice sampling algorithm by P. Schär, M. Habeck, and D. Rudolf [SHR2023].\n\nArguments\n\nw::Real: Initial window size for the radius shrinkage procedure.\n\nKeyword Arguments\n\nw::Real: Initial window size for the radius shrinkage procedure\nmax_proposals::Int: Maximum number of proposals allowed until throwing an error (default: 10000).\n\ninfo: Info\nBy the nature of polar coordinates, GPSS only works reliably for targets with dimension at least d geq 2.\n\ninfo: Info\nThe initial window size w must be set at least an order of magnitude larger than what is sensible for other slice samplers. Otherwise, a large number of rejections might be experienced.\n\nwarning: Warning\nWhen initializing the chain (e.g. the initial_params keyword arguments in AbstractMCMC.sample), it is necessary to initialize from a point x_0 that has a sensible norm lVert x_0 rVert  0, otherwise, the chain will start from a pathological point in polar coordinates. This might even result in the sampler getting stuck in an infinite loop. (This can be prevented by setting max_proposals.) If lVert x_0 rVert leq 10^-5, the current implementation will display a warning. \n\n\n\n\n\n","category":"type"},{"location":"meta_multivariate/#meta","page":"Meta Multivariate Samplers","title":"Meta Multivariate Samplers","text":"To use univariate slice sampling strategies on targets with more than on dimension, one has to embed them into a \"meta\" multivariate sampling scheme that relies on univariate sampling elements. The two most popular approaches for this are Gibbs sampling[GG1984] and hit-and-run[BRS1993].","category":"section"},{"location":"meta_multivariate/#Random-Permutation-Gibbs","page":"Meta Multivariate Samplers","title":"Random Permutation Gibbs","text":"Gibbs sampling[GG1984] is a strategy where we sample from the posterior one coordinate at a time, conditioned on the values of all other coordinates. In practice, one can pick the coordinates in any order they want as long as it does not depend on the state of the chain. It is generally hard to know a-prior which \"scan order\" is best, but randomly picking coordinates tend to work well in general. Currently, we only provide random permutation scan, which guarantees that all coordinates are updated at least once after d transitions. At the same time, reversibility is maintained by randomly permuting the order we go through each coordinate:\n\nEach call to AbstractMCMC.step internally performs d Gibbs transition so that all coordinates are updated.\n\nFor example:\n\nRandPermGibbs(SliceSteppingOut(2.))\n\nIf one wants to use a different slice sampler configuration for each coordinate, one can mix-and-match by passing a Vector of slice samplers, one for each coordinate. For instance, for a 2-dimensional target:\n\nRandPermGibbs([SliceSteppingOut(2.; max_proposals=32), SliceDoublingOut(2.),])","category":"section"},{"location":"meta_multivariate/#Hit-and-Run","page":"Meta Multivariate Samplers","title":"Hit-and-Run","text":"Hit-and-run is a simple meta algorithm where we sample over a random 1-dimensional projection of the space. That is, at each iteration, we sample a random direction\n\n    theta_n sim operatornameUniform(mathbbS^d-1)\n\nand perform a Markov transition along the 1-dimensional subspace\n\nbeginaligned\n    lambda_n sim pleft(lambda mid x_n-1 theta_n right) propto pileft( x_n-1 + lambda theta_n right) \n    x_n = x_n-1 + lambda_n theta_n\nendaligned\n\nwhere pi is the target unnormalized density. Applying slice sampling for the 1-dimensional subproblem has been popularized by David Mackay[M2003], and is, technically, also a Gibbs sampler.  (Or is that Gibbs samplers are hit-and-run samplers?) Unlike RandPermGibbs, which only makes axis-aligned moves, HitAndRun can choose arbitrary directions, which could be helpful in some cases.\n\nThis can be used, for example, as follows:\n\nHitAndRun(SliceSteppingOut(2.))\n\nUnlike RandPermGibbs, HitAndRun does not provide the option of using a unique unislice object for each coordinate. This is a natural limitation of the hit-and-run sampler: it does not operate on individual coordinates.\n\n[GG1984]: Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6).\n\n[BRS1993]: Bélisle, C. J., Romeijn, H. E., & Smith, R. L. (1993). Hit-and-run algorithms for generating multivariate distributions. Mathematics of Operations Research, 18(2), 255-266.\n\n[M2003]: MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge university press.","category":"section"},{"location":"meta_multivariate/#SliceSampling.RandPermGibbs","page":"Meta Multivariate Samplers","title":"SliceSampling.RandPermGibbs","text":"RandPermGibbs(unislice)\n\nRandom permutation coordinate-wise Gibbs sampling strategy. This applies unislice coordinate-wise in a random order.\n\nArguments\n\nunislice::Union{<:AbstractUnivariateSliceSampling,<:AbstractVector{<:AbstractUnivariateSliceSampling}}: a single or a vector of univariate slice sampling algorithms.\n\nWhen unislice is a vector of samplers, each slice sampler is applied to the corresponding coordinate of the target posterior. In that case, the length(unislice) must match the dimensionality of the posterior.\n\n\n\n\n\n","category":"type"},{"location":"meta_multivariate/#SliceSampling.HitAndRun","page":"Meta Multivariate Samplers","title":"SliceSampling.HitAndRun","text":"HitAndRun(unislice)\n\nHit-and-run sampling strategy[BRS1993]. This applies unislice along a random direction uniform sampled from the sphere.\n\nArguments\n\nunislice::AbstractUnivariateSliceSampling: Univariate slice sampling algorithm.\n\n\n\n\n\n","category":"type"},{"location":"univariate_slice/#Univariate-Slice-Sampling-Algorithms","page":"Univariate Slice Sampling","title":"Univariate Slice Sampling Algorithms","text":"","category":"section"},{"location":"univariate_slice/#Introduction","page":"Univariate Slice Sampling","title":"Introduction","text":"These algorithms are the \"single-variable\" slice sampling algorithms originally described by Neal[N2003]. Since these algorithms are univariate, one has to incorporate them into a \"meta\" multivariate sampler, which are discussed in this section.","category":"section"},{"location":"univariate_slice/#Fixed-Initial-Interval-Slice-Sampling","page":"Univariate Slice Sampling","title":"Fixed Initial Interval Slice Sampling","text":"This is the most basic form of univariate slice sampling, where the proposals are generated within a fixed interval formed by the window.","category":"section"},{"location":"univariate_slice/#Adaptive-Initial-Interval-Slice-Sampling","page":"Univariate Slice Sampling","title":"Adaptive Initial Interval Slice Sampling","text":"These algorithms try to adaptively set the initial interval through a simple search procedure. The \"stepping-out\" procedure grows the initial window on a linear scale, while the \"doubling-out\" procedure grows it geometrically. window controls the scale of the increase.","category":"section"},{"location":"univariate_slice/#What-Should-I-Use?","page":"Univariate Slice Sampling","title":"What Should I Use?","text":"This highly depends on the problem at hand. In general, the doubling-out procedure tends to be more expensive as it requires additional log-target evaluations to decide whether to accept a proposal. However, if the scale of the posterior varies drastically, doubling out might work better. In general, it is recommended to use the stepping-out procedure.\n\n[N2003]: Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31(3), 705-767.","category":"section"},{"location":"univariate_slice/#SliceSampling.Slice","page":"Univariate Slice Sampling","title":"SliceSampling.Slice","text":"Slice(window; max_proposals)\n\nUnivariate slice sampling with a fixed initial interval (Scheme 2 by Neal[N2003])\n\nArguments\n\nwindow::Real: Proposal window.\n\nKeyword Arguments\n\nmax_proposals::Int: Maximum number of proposals allowed until throwing an error (default: 10000).\n\n\n\n\n\n","category":"type"},{"location":"univariate_slice/#SliceSampling.SliceSteppingOut","page":"Univariate Slice Sampling","title":"SliceSampling.SliceSteppingOut","text":"SliceSteppingOut(window; max_stepping_out, max_proposals)\n\nUnivariate slice sampling by automatically adapting the initial interval through the \"stepping-out\" procedure (Scheme 3 by Neal[N2003])\n\nArguments\n\nwindow::Real: Proposal window.\n\nKeyword Arguments\n\nmax_stepping_out::Int: Maximum number of \"stepping outs\" (default: 32).\nmax_proposals::Int: Maximum number of proposals allowed until throwing an error (default: 10000).\n\n\n\n\n\n","category":"type"},{"location":"univariate_slice/#SliceSampling.SliceDoublingOut","page":"Univariate Slice Sampling","title":"SliceSampling.SliceDoublingOut","text":"SliceDoublingOut(window; max_doubling_out, max_proposals)\n\nUnivariate slice sampling by automatically adapting the initial interval through the \"doubling-out\" procedure (Scheme 4 by Neal[N2003])\n\nArguments\n\nwindow::Real: Proposal window.\n\nKeyword Arguments\n\nmax_doubling_out: Maximum number of \"doubling outs\" (default: 8).\nmax_proposals::Int: Maximum number of proposals allowed until throwing an error (default: 10000).\n\n\n\n\n\n","category":"type"},{"location":"general/#General-Usage","page":"General Usage","title":"General Usage","text":"This package implements the AbstractMCMC interface. AbstractMCMC provides a unifying interface for MCMC algorithms applied to LogDensityProblems.","category":"section"},{"location":"general/#Examples","page":"General Usage","title":"Examples","text":"","category":"section"},{"location":"general/#Drawing-Samples-From-a-LogDensityProblems-Through-AbstractMCMC","page":"General Usage","title":"Drawing Samples From a LogDensityProblems Through AbstractMCMC","text":"SliceSampling.jl implements the AbstractMCMC interface through LogDensityProblems. That is, one simply needs to define a LogDensityProblems and pass it to AbstractMCMC:\n\nusing AbstractMCMC\nusing Distributions\nusing LinearAlgebra\nusing LogDensityProblems\nusing Plots\n\nusing SliceSampling\n\nstruct Target{D}\n\tdist::D\nend\n\nLogDensityProblems.logdensity(target::Target, x) = logpdf(target.dist, x)\n\nLogDensityProblems.dimension(target::Target) = length(target.distx)\n\nLogDensityProblems.capabilities(::Type{<:Target}) = LogDensityProblems.LogDensityOrder{0}()\n\nsampler         = GibbsPolarSlice(2.0)\nn_samples       = 10000\nmodel           = Target(MvTDist(5, zeros(10), Matrix(I, 10, 10)))\nlogdensitymodel = AbstractMCMC.LogDensityModel(model)\n\nchain   = sample(logdensitymodel, sampler, n_samples; initial_params=randn(10))\nsamples = hcat([transition.params for transition in chain]...)\n\nplot(samples[1,:], xlabel=\"Iteration\", ylabel=\"Trace\")\nsavefig(\"abstractmcmc_demo.svg\")\n\n(Image: )","category":"section"},{"location":"general/#Drawing-Samples-From-Turing-Models","page":"General Usage","title":"Drawing Samples From Turing Models","text":"SliceSampling.jl can also be used to sample from Turing models through Turing's externalsampler interface:\n\nusing Distributions\nusing Turing\nusing SliceSampling\n\n@model function demo()\n    s ~ InverseGamma(3, 3)\n    m ~ Normal(0, sqrt(s))\nend\n\nsampler   = RandPermGibbs(SliceSteppingOut(2.))\nn_samples = 10000\nmodel     = demo()\nchain     = sample(model, externalsampler(sampler), n_samples; progress=false)\ndescribe(chain)","category":"section"},{"location":"general/#Conditional-sampling-in-a-Turing.Gibbs-sampler","page":"General Usage","title":"Conditional sampling in a Turing.Gibbs sampler","text":"SliceSampling.jl be used as a conditional sampler in Turing.Gibbs.\n\nusing Distributions\nusing Turing\nusing SliceSampling\n\n@model function simple_choice(xs)\n    p ~ Beta(2, 2)\n    z ~ Bernoulli(p)\n    for i in 1:length(xs)\n        if z == 1\n            xs[i] ~ Normal(0, 1)\n        else\n            xs[i] ~ Normal(2, 1)\n        end\n    end\nend\n\nsampler = Turing.Gibbs(\n    :p => externalsampler(SliceSteppingOut(2.0)),\n    :z => PG(20),\n)\n\nn_samples = 1000\nmodel     = simple_choice([1.5, 2.0, 0.3])\nchain     = sample(model, sampler, n_samples; progress=false)\ndescribe(chain)","category":"section"},{"location":"general/#Drawing-Samples","page":"General Usage","title":"Drawing Samples","text":"For drawing samples using the algorithms provided by SliceSampling, the user only needs to call:\n\nsample([rng,] model, slice, N; initial_params)\n\nslice::AbstractSliceSampling: Any slice sampling algorithm provided by SliceSampling.\nmodel: A model implementing the LogDensityProblems interface.\nN: The number of samples\n\nThe output is a vector of SliceSampling.Transitions, which contains the following:\n\nFor the keyword arguments, SliceSampling allows:\n\ninitial_params: The initial state of the Markov chain (default: nothing).\n\nIf initial_params is nothing, the following function can be implemented to provide an initialization:","category":"section"},{"location":"general/#Performing-a-Single-Transition","page":"General Usage","title":"Performing a Single Transition","text":"For more fined-grained control, the user can call AbstractMCMC.step. That is, the chain can be initialized by calling:\n\ntransition, state = AbstractMCMC.steps([rng,] model, slice; initial_params)\n\nand then each MCMC transition on state can be performed by calling:\n\ntransition, state = AbstractMCMC.steps([rng,] model, slice, state)\n\nFor more details, refer to the documentation of AbstractMCMC.","category":"section"},{"location":"general/#SliceSampling.Transition","page":"General Usage","title":"SliceSampling.Transition","text":"struct Transition\n\nStruct containing the results of the transition.\n\nFields\n\nparams: Samples generated by the transition.\nlp::Real: Log-target density of the samples.\ninfo::NamedTuple: Named tuple containing information about the transition. \n\n\n\n\n\n","category":"type"},{"location":"general/#SliceSampling.initial_sample","page":"General Usage","title":"SliceSampling.initial_sample","text":"initial_sample(rng, model)\n\nReturn the initial sample for the model using the random number generator rng.\n\nArguments\n\nrng::Random.AbstractRNG: Random number generator.\nmodel: The target LogDensityProblem.\n\n\n\n\n\n","category":"function"},{"location":"#SliceSampling","page":"Home","title":"SliceSampling","text":"This package implements slice sampling algorithms.  Slice sampling finds its roots in the Swendsen–Wang algorithm for Ising models[SW1987][ES1988]. It later came into the interest of the statistical community through Besag and Green[BG1993], and popularized by Neal [N2003]. Furthermore, Neal introduced various ways to efficiently implement slice samplers. This package provides the original slice sampling algorithms by Neal and their later extensions.\n\n[SW1987]: Swendsen, R. H., & Wang, J. S. (1987). Nonuniversal critical dynamics in Monte Carlo simulations. Physical review letters, 58(2), 86.\n\n[ES1988]: Edwards, R. G., & Sokal, A. D. (1988). Generalization of the fortuin-kasteleyn-swendsen-wang representation and monte carlo algorithm. Physical review D, 38(6), 2009.\n\n[BG1993]: Besag, J., & Green, P. J. (1993). Spatial statistics and Bayesian computation. Journal of the Royal Statistical Society Series B: Statistical Methodology, 55(1), 25-37.\n\n[N2003]: Neal, R. M. (2003). Slice sampling. The annals of statistics, 31(3), 705-767.\n\n","category":"section"}]
}
